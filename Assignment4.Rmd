---
title: "Assignment4"
author: "Holly Finertie - HF2379"
date: "Due: 2/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(mgcv)
library(factoextra)
library(stats)
library(cluster)

set.seed(100)
```

## Part I: Implementing a Simple Prediction Pipeline

### Data Import
I converted categorical variables into factors and cleaned the names. I decided not omit missing values. 

```{r}
nychealth = read_csv("./data/class4_p1.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    chronic1 = as.factor(chronic1), 
    chronic3 = as.factor(chronic3), 
    chronic4 = as.factor(chronic4), 
    tobacco1 = as.factor(tobacco1), 
    alcohol1 = as.factor(alcohol1), 
    habits5 = as.factor(habits5), 
    habits7 = as.factor(habits7), 
    agegroup = as.factor(agegroup),
    dem3 = as.factor(dem3),
    dem4 = as.factor(dem4), 
    dem8 = as.factor(dem8),
    povertygroup = as.factor(povertygroup)
  )

head(nychealth)
```


### Testing and Training Data Sets
Set up a testing and training data set with 70% vs 30%. 
```{r}
train_nychealth = nychealth %>% dplyr::sample_frac(.7)
test_nychealth = dplyr::anti_join(nychealth, train_nychealth, by = 'x1')
```

### (1) Fit Prediction Models Using Training Data Sets

Model1: $$ healthdays = \beta_0 + \beta_1 Chronic1 + \beta_2 Chronic3 + \beta_3 Chronic4 + \beta_4 BMI + \beta_5 Tobacco1 + \beta_6 Alcohol1 + \beta_7 PovertyGroup $$

Model2: $$ healthdays = \beta_0 + \beta_1 Chronic1 + \beta_2 BMI +  \beta_3 PovertyGroup $$
```{r}
model1 = lm(healthydays ~ chronic1 + chronic3 + chronic4 + bmi + tobacco1 + alcohol1 + povertygroup, data = train_nychealth)

model1 %>% broom::tidy() %>% knitr::kable()

model2 = lm(healthydays ~ chronic1 + bmi + povertygroup, data = train_nychealth)

model2 %>% broom::tidy() %>% knitr::kable()
```

### (2) Apply Models to Testing Data Set
Using RMSE as a measure of fit, the first model is the preferred model of prediction. 

Model 1 RMSE = `r round(rmse(model1, test_nychealth), digits = 3)`
Model 2 RMSE = `r round(rmse(model2, test_nychealth), digits = 3)`

```{r}
rmse(model1, test_nychealth)
rmse(model2, test_nychealth)
```

### (3) Implementation of Final Model 

Final Model: $$ healthdays = \beta_0 + \beta_1 Chronic1 + \beta_2 Chronic3 + \beta_3 Chronic4 + \beta_4 BMI + \beta_5 Tobacco1 + \beta_6 Alcohol1 + \beta_7 PovertyGroup $$


## Part II: Conducting an Unsupervised Analysis

### Data Import

```{r}
data("USArrests") 

arrestdata = USArrests %>% 
  janitor::clean_names() %>% 
  drop_na()
```

### (4) Hierarchical Clustering: Agglomeration Method
I used Euclidean distance for the dissimilarity matrix and then completed 3 difference agglomeration methods: complete, average, and single. 

#### Complete Agglomeration Method
The complete method uses the largest or maximum pairwise dissimilarity as the distance between two clusters. 
```{r}
diss.matrix = dist(arrestdata, method = "euclidean")

complete_link = hclust(diss.matrix, method = "complete")
plot(complete_link, cex = 0.6, hang = -1)
```

#### Average Agglomeration Method
The average method computes all pairwise dissimilarities between each clusters and uses the average dissimilarities as the distance between two clusters.

```{r}
average_link = hclust(diss.matrix, method = "average")
plot(average_link, cex = 0.6, hang = -1)
```

#### Single Agglomeration Method
The complete method uses the smallest pairwise dissimilarity as the distance between two clusters. 
```{r}
single_link = hclust(diss.matrix, method = "single")
plot(single_link, cex = 0.6, hang = -1)
```

#### Difference Between Dendograms
The complete and average cluster dendrograms are visually pretty similar; however, the complete dendogram has larger height because we used the largest maximum distance between clusters. The single cluster dendrogram has the shortest hieght because we used the shortest distance between clusters. It is also much longer and less compact than the average and complete agglomeration methods. 

#### Optimal Clusters
Using the gap statistic below, we determine that ther optimal number of clusters is 4. 

```{r}
gap_stat = clusGap(arrestdata, FUN = hcut,nstart = 25, K.max = 10, B = 50)
print(gap_stat, method = "firstmax")
fviz_gap_stat(gap_stat)
```

### (5) Reserach Question
The USArrests data set includes arrests per 100,000 for each of hte 50 states from 1973. If this was 2020 data, I would review the association between demographic variables and arrest clustering across states. 

*Question*: How are SES, race, and education variables associated with arrest clustering across states in 2020? 


